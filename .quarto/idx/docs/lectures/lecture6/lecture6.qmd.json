{"title":"DSC 3091- Advanced Statistics Applications I","markdown":{"yaml":{"title":"DSC 3091- Advanced Statistics Applications I","subtitle":"Penalized Regression and LARS algorithm","author":"Dr Jagath Senarathne","institute":"Department of Statistics and Computer Science","format":{"revealjs":{"theme":["beige","custom.scss"],"slide-number":true,"chalkboard":{"theme":"whiteboard"}}},"editor":"visual"},"headingText":"Penalized Regression","containsRefs":false,"markdown":"\n\n\n-   Use the concept of penalizing or reducing the impact of certain variables in the model.\n\n-   More suitable for large multivariate data sets containing a number of variables superior to the number of samples.\n\n-   Also useful for selecting key variables from a large set of variables.\n\n-   Penalized regression models improve prediction in new data by shrinking the size of coefficients and retaining those with coefficients greater than zero.\n\n# Penalized regression methods\n\n-   Includes a penalty term to reduce (i.e. shrink) the coefficient values towards zero.\n\n-   As such, the variables with minor contribution to the outcome have their coefficients close to zero.\n\n-   Shrinkage methods:\n\n    1.  Ridge Regression\n\n    2.  Lasso Regression\n\n    3.  Elastic net Regression\n\n# Ridge Regression\n\n-   We use ridge regression to tackle the multicollinearity problem.\n\n-   Due to multicollinearity, the model estimates (least square) see a large variance.\n\n-   Ridge regression is a method by which we add a degree of bias to the regression estimates.\n\n-   Ridge regression performs L2 regularization which adds a penalty equivalent to the sum of the squared coefficients and tries to minimize them.\n\n------------------------------------------------------------------------\n\n-   The ridge regression minimizes,\n\n$\\sum(Y_i-\\hat{Y}_i)^2+\\lambda\\sum{\\beta_j^2}$\n\n![](images/paste-4F17340D.png)\n\n------------------------------------------------------------------------\n\n## Build a ridge regression model in r\n\n-   To build the ridge regression in r, we use `glmnet` function from glmnet package in R.\n\n::: callout-note\n## Example\n\nLet's consider the mtcars dataset in R, and fit a ridge regression model to predict the mileage of the car.\n:::\n\n------------------------------------------------------------------------\n\n```{r, echo=T}\nlibrary(glmnet)\nx_var <- data.matrix(mtcars[, c(\"hp\", \"wt\", \"drat\")])\ny_var <- mtcars[, \"mpg\"]\nlambda_seq <- 10^seq(2, -2, by = -.1)\nfit <- glmnet(x_var, y_var, alpha = 0, lambda  = lambda_seq)\nsummary(fit)\n```\n\n------------------------------------------------------------------------\n\n```{r, echo=T}\n# Get coefficients of all 100 models\nridge_coef <- coef(fit)\n\n# Display coefficients for 7 models. \nround(ridge_coef[, c(1:3, 38:41)], 3)\n```\n\n------------------------------------------------------------------------\n\n```{r}\nCoef_data=data.frame(lambda=10^seq(2, -2, by = -.1))\n\nCoef_data$Tntercept=as.numeric(ridge_coef[1,])\nCoef_data$hp=as.numeric(ridge_coef[2,])\nCoef_data$wt=as.numeric(ridge_coef[3,])\nCoef_data$drat=as.numeric(ridge_coef[4,])\nstack_Y=stack(Coef_data[,3:5])\nnames(stack_Y)=c(\"Coefficient\",\"Variable\")\nstack_Y$lambda=rep(lambda_seq,3)\nlibrary(ggplot2)\n\nggplot(data=stack_Y, aes(x=lambda,y=Coefficient, group=Variable,color=Variable)) +\n  geom_line()\n```\n\n------------------------------------------------------------------------\n\n-   We can also produce a Trace plot to visualize how the coefficient estimates changed as a result of increasing $\\lambda$.\n\n```{r, echo=T}\nplot(fit, xvar =\"lambda\")\n```\n\n## Choose an Optimal Value for $\\lambda$\n\n-   Identify the lambda value that produces the lowest mean squared error (MSE) by using k-fold cross-validation.\n\n-   `glmnet` has the function `cv.glmnet()` that performs k-fold cross validation using k = 10 folds.\n\n```{r, echo=T}\nset.seed(123)\n#perform k-fold cross-validation to find optimal lambda value\ncv_model <- cv.glmnet(x_var, y_var, alpha = 0)\n\n#find optimal lambda value that minimizes test MSE\nbest_lambda <- cv_model$lambda.min\nbest_lambda\n```\n\n------------------------------------------------------------------------\n\n## Plot of MSE by lambda value\n\n```{r, echo=T}\nplot(cv_model)\n```\n\n------------------------------------------------------------------------\n\n## Final model\n\n```{r,echo=T}\nfinal_model <- glmnet(x_var, y_var, alpha = 0, lambda = best_lambda)\ncoef(final_model)\n\n```\n\n------------------------------------------------------------------------\n\n## R-squared of the model\n\n```{r, echo=T}\ny_predicted <- predict(final_model, s = best_lambda, newx = x_var)\n\n#find SST and SSE\nsst <- sum((y_var - mean(y_var))^2)\nsse <- sum((y_predicted - y_var)^2)\n\n#find R-Squared\nrsq <- 1 - sse/sst\nrsq\n```\n\n------------------------------------------------------------------------\n\n-   `lmridge` package can also be used to fit ridge regression models in R.\n\n    ![](images/paste-5F48DD3D.png)\n\n    <https://journal.r-project.org/archive/2018/RJ-2018-060/RJ-2018-060.pdf>\n\n# Lasso regression\n\n-   Lasso stands for Least Absolute Shrinkage and Selection Operator.\n\n-   It shrinks the regression coefficients toward zero by penalizing the regression model with a penalty term called L1-norm.\n\n$\\sum(Y_i-\\hat{Y}_i)^2+\\lambda\\sum{|\\beta_j|}$\n\n------------------------------------------------------------------------\n\n-   One advantage of lasso regression over ridge regression, is that it produces simpler and more interpretable models that incorporate only a reduced set of the predictors.\n\n-   However, neither ridge regression nor the lasso will universally dominate the other.\n\n-   Generally, lasso might perform better in a situation where some of the predictors have large coefficients, and the remaining predictors have very small coefficients.\n\n-   Ridge regression will perform better when the outcome is a function of many predictors, all with coefficients of roughly equal size.\n\n------------------------------------------------------------------------\n\n## Computing lasso regression in R\n\n```{r, echo=T}\n# Find the best lambda using cross-validation\nset.seed(123) \ncvl <- cv.glmnet(x_var, y_var, alpha = 1)\n# Display the best lambda value\ncvl$lambda.min\n```\n\n```{r, echo=T}\n# Fit the final model on the training data\nmodel_lasso <- glmnet(x_var, y_var, alpha = 1, lambda = cvl$lambda.min)\n# Dsiplay regression coefficients\ncoef(model_lasso)\n```\n\n------------------------------------------------------------------------\n\n## R-squared of the lasso regression model\n\n```{r, echo=T}\ny_predicted <- predict(model_lasso, s = cvl$lambda.min, newx = x_var)\n\n#find SST and SSE\nsst <- sum((y_var - mean(y_var))^2)\nsse <- sum((y_predicted - y_var)^2)\n\n#find R-Squared\nrsq <- 1 - sse/sst\nrsq\n```\n\n# Elastic net regession\n\n-   Elastic Net produces a regression model that is penalized with both the L1-norm and L2-norm.\n\n-   The objective of this method is to effectively shrink coefficients (like in ridge regression) and to set some coefficients to zero (as in LASSO).\n\n-   The elastic net regression can be easily computed using the `caret` workflow, which invokes the `glmnet` package.\n\n-   The caret packages tests a range of possible alpha and lambda values, then selects the best values for lambda and alpha, resulting to a final model that is an elastic net model.\n\n------------------------------------------------------------------------\n\n## How to find alpha and lambda?\n\n```{r, echo=T}\nlibrary(caret)\ndata.new <- data.frame(y_var,x_var)\nset.seed(123)\nmodel_ER <- train(\n  y_var~.,data=data.new, method = \"glmnet\",\n  trControl = trainControl(\"cv\", number = 10),\n  tuneLength = 10\n)\n# Best tuning parameter\nmodel_ER$bestTune\n```\n\n------------------------------------------------------------------------\n\n-   Coefficient of the final model:\n\n```{r, echo=T}\ncoef(model_ER$finalModel, model_ER$bestTune$lambda)\n```\n\n------------------------------------------------------------------------\n\n## R-squared of the elastic net regression model\n\n```{r, echo=T}\ny_predicted <- predict(model_ER, newx = x_var)\n\n#find SST and SSE\nsst <- sum((y_var - mean(y_var))^2)\nsse <- sum((y_predicted - y_var)^2)\n\n#find R-Squared\nrsq <- 1 - sse/sst\nrsq\n```\n\n# LARS algorithm\n\n-   Least-angle regression (LARS) is an algorithm for fitting linear regression models to high-dimensional data.\n\n-   It is a model selection method for linear regression.\n\n-   At each step, LARS finds the attribute which is most highly correlated to the target value.\n\n------------------------------------------------------------------------\n\n## Algorithm\n\n1.  Normalize all values to have zero mean and unit variance.\n\n2.  Find a variable that is most highly correlated to the residual. Move the regression line in this direction until we reach another variable that has the same or higher correlation.\n\n3.  When we have two variables that have the same correlation, move the regression line at an angle that is in between (i.e., least angle between the two variables).\n\n4.  Continue this until all of our data is exhausted or until you think the model is big and 'general' enough.\n\n------------------------------------------------------------------------\n\n## Computing LARS model in R\n\n-   Can use the `lars` function from `lars` package in R.\n\n`lars(x, y, type = c(\"lasso\", \"lar\", \"forward.stagewise\", \"stepwise\"), trace = FALSE, normalize = TRUE, intercept = TRUE, Gram, eps = 1e-12, max.steps, use.Gram = TRUE)`\n\n------------------------------------------------------------------------\n\n## Example\n\n```{r, echo=TRUE}\nlibrary(lars)\nLars_obj <- lars(x_var,y_var,type=\"lar\")\nLars_obj\n```\n\n```{r, echo=T}\nplot(Lars_obj)\n```\n\n------------------------------------------------------------------------\n\n```{r}\npar(mfrow=c(2,2))\nobject <- lars(x_var,y_var)\nplot(object)\nobject2 <- lars(x_var,y_var,type=\"lar\")\nplot(object2)\nobject3 <- lars(x_var,y_var,type=\"for\") \nplot(object3)\nobject4 <- lars(x_var,y_var,type=\"stepwise\") \nplot(object4)\n\n```\n\n------------------------------------------------------------------------\n\n### Advantages of using LARS:\n\n-   Computationally as fast as forward selection but may sometimes be more accurate.\n\n-   Numerically very efficient when the number of features is much larger than the number of data instances.\n\n-   It can easily be modified to produce solutions for other estimators.\n\n### Disadvantages of using LARS:\n\n-   LARS is highly sensitive to noise and can produce unpredictable results sometimes.\n\n------------------------------------------------------------------------\n\n::: callout-important\n## IN-CLASS ASSIGNMENT\n\nObtain a penalized regression model that best describes the variations in the response variable `medv` from the `Boston` dataset in `MASS` package.\n:::\n\n# Some Useful Links ...\n\n-   <http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/>\n\n-   <https://www.r-bloggers.com/2020/05/simple-guide-to-ridge-regression-in-r/>\n\n-   <https://www.pluralsight.com/guides/linear-lasso-and-ridge-regression-with-r>\n\n-   <https://rpubs.com/beane/n6_1>\n\n-   <https://cran.r-project.org/web/packages/lars/lars.pdf>\n"},"formats":{"revealjs":{"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","css":["../lectures.css"],"output-file":"lecture6.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.0.38","auto-stretch":true,"editor":"visual","sidebar":"lectures","search":false,"title":"DSC 3091- Advanced Statistics Applications I","subtitle":"Penalized Regression and LARS algorithm","author":"Dr Jagath Senarathne","institute":"Department of Statistics and Computer Science","theme":["beige","custom.scss"],"slideNumber":true,"chalkboard":{"theme":"whiteboard"}}}}}