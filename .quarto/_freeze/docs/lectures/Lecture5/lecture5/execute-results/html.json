{
  "hash": "c97ffe912b447ba81fd1e00f9a8b12a2",
  "result": {
    "markdown": "---\ntitle: DSC 3091- Advanced Statistics Applications I\nsubtitle: \"Multiple Linear Regression\"\nauthor: Dr Jagath Senarathne\ninstitute: Department of Statistics and Computer Science \nformat: \n  revealjs:\n      theme: [beige, custom.scss]\n      slide-number: true\n      chalkboard: \n        theme: whiteboard\n      \neditor: visual\n---\n\n\n# Multiple Linear Regression\n\n-   Studying the variation of Y(response variable) as a function of more than one X(explanatory) variables.\n\n-   The general multiple linear regression model;\n\n\n$$ Y= \\beta_0+\\beta_1x_1+\\beta_2x_2...+\\beta_px_p+ \\epsilon$$\n\n\nWhere, $\\epsilon \\sim N(0,\\sigma^2)$ is the random noise component of the model and $\\beta_0,\\beta_1,...,\\beta_p$, are the unknown parameters.\n\n------------------------------------------------------------------------\n\n::: callout-note\n## Example 1\n\nYou might want to predict how well a stock will do based on some other information that you just happen to have.\n:::\n\n::: callout-note\n## Example 2\n\nImagine that you are a tourist guide. You need to provide the price range of food to your clients. The price of those food usually correlates with the Food Quality and Service Quality of the Restaurant.\n:::\n\n::: callout-note\n## Example 3\n\nImagine that you're a traffic planner in your city and need to estimate the average commute time of drivers going from the east side of the city to the west. You know that it will depend on a number of factors like the distance driven, the number of stoplights on the route, and the number of other vehicles on the road.\n:::\n\n------------------------------------------------------------------------\n\n## How to fit a multiple linear regression model?\n\n<br>\n\n**Steps you can follow:**\n\n-   Graphical Interpretation\n-   Parameter Estimation\n-   Tests on Parameters\n-   Analysis of Variance\n-   Interpretation and Prediction\n\n------------------------------------------------------------------------\n\n## Graphical Interpretation\n\n-   Before fitting a multiple liner regression model, plot a scatterplot between every possible pair of variables.\n\n-   It helps in visualizing the strength of relationships.\n\n-   Can also be used for variable selection and identifying co-linearity.\n\n::: callout-note\n## Example\n\nLet's consider the built-in dataset, \"abalone\", in the package called \"AppliedPredictiveModeling\". Abalone data explains various physical measurements of sea snails. The main objective of this study is to predict the number of rings(which is an indicator of the age of the snail) using the other variables.\n:::\n\n------------------------------------------------------------------------\n\n## Example Cont...\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"AppliedPredictiveModeling\")\nlibrary(AppliedPredictiveModeling)\ndata(abalone)\nattach(abalone)\nhead(abalone)\nsummary(abalone)\ncor(abalone[,-1])\n```\n:::\n\n\n<br>\n\n### Draw a correlogram using corrplot() function\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"corrplot\")\nlibrary(corrplot)\ncorrplot(cor(abalone[,-1]), tl.col = \"red\")\n```\n:::\n\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture5_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Draw scatter plots using chart.Correlation():\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"PerformanceAnalytics\")\nlibrary(\"PerformanceAnalytics\")\nchart.Correlation(abalone[,-1], histogram=TRUE, pch=19)\n```\n\n::: {.cell-output-display}\n![](lecture5_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n## Parameter Estimation\n\n-   Similar to the simple linear regression, we can use `lm()` function to estimate the unknown parameters $\\beta_0, \\beta_1,.,\\beta_p$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1 <-  lm(Rings~as.factor(Type) +LongestShell+ Diameter+ WholeWeight+ ShuckedWeight+ VisceraWeight+ ShellWeight)\nmodel1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Rings ~ as.factor(Type) + LongestShell + Diameter + \n    WholeWeight + ShuckedWeight + VisceraWeight + ShellWeight)\n\nCoefficients:\n     (Intercept)  as.factor(Type)I  as.factor(Type)M      LongestShell  \n         4.24786          -0.87919           0.04437          -0.05352  \n        Diameter       WholeWeight     ShuckedWeight     VisceraWeight  \n        12.73487           9.06362         -19.95631         -10.18938  \n     ShellWeight  \n         9.57238  \n```\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Rings ~ as.factor(Type) + LongestShell + Diameter + \n    WholeWeight + ShuckedWeight + VisceraWeight + ShellWeight)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.1813 -1.3130 -0.3437  0.8690 13.7654 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        4.24786    0.28883  14.707  < 2e-16 ***\nas.factor(Type)I  -0.87919    0.10269  -8.562  < 2e-16 ***\nas.factor(Type)M   0.04437    0.08380   0.529    0.597    \nLongestShell      -0.05352    1.81860  -0.029    0.977    \nDiameter          12.73487    2.22738   5.717 1.16e-08 ***\nWholeWeight        9.06362    0.72947  12.425  < 2e-16 ***\nShuckedWeight    -19.95631    0.82169 -24.287  < 2e-16 ***\nVisceraWeight    -10.18938    1.29997  -7.838 5.76e-15 ***\nShellWeight        9.57238    1.12490   8.510  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.207 on 4168 degrees of freedom\nMultiple R-squared:  0.5324,\tAdjusted R-squared:  0.5315 \nF-statistic: 593.3 on 8 and 4168 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Let's create a dummy variable!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nabalone$Type.I=ifelse(abalone$Type==\"I\",1,0)\nabalone$Type.M=ifelse(abalone$Type==\"M\",1,0)\nmodel2 <-  lm(Rings~Type.I + Type.M+ LongestShell+ Diameter+ WholeWeight+ ShuckedWeight+ VisceraWeight+ ShellWeight,data=abalone)\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Rings ~ Type.I + Type.M + LongestShell + Diameter + \n    WholeWeight + ShuckedWeight + VisceraWeight + ShellWeight, \n    data = abalone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.1813 -1.3130 -0.3437  0.8690 13.7654 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     4.24786    0.28883  14.707  < 2e-16 ***\nType.I         -0.87919    0.10269  -8.562  < 2e-16 ***\nType.M          0.04437    0.08380   0.529    0.597    \nLongestShell   -0.05352    1.81860  -0.029    0.977    \nDiameter       12.73487    2.22738   5.717 1.16e-08 ***\nWholeWeight     9.06362    0.72947  12.425  < 2e-16 ***\nShuckedWeight -19.95631    0.82169 -24.287  < 2e-16 ***\nVisceraWeight -10.18938    1.29997  -7.838 5.76e-15 ***\nShellWeight     9.57238    1.12490   8.510  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.207 on 4168 degrees of freedom\nMultiple R-squared:  0.5324,\tAdjusted R-squared:  0.5315 \nF-statistic: 593.3 on 8 and 4168 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n# Methods of building a multiple linear regression model\n\n-   Backward Elimination\n-   Forward Selection\n-   Bidirectional Elimination (Stepwise regression)\n\n## Backward elimination\n\n1.  Set a significance level for which data will stay in the model.\n\n2.  Next, fit the full model with all possible predictors.\n\n3.  Consider the predictor with the highest P-value. If the P-value is greater than the significance level, you'll move to step four, otherwise, you're done!\n\n4.  Remove that predictor with the highest P-value.\n\n5.  Fit the model without that predictor variable.\n\n6.  Go back to step 3, do it all over, and keep doing that until you come to a point where even the highest P-value is \\< SL.\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <-  lm(Rings~Type.I + Type.M+ LongestShell+ Diameter+ WholeWeight+ ShuckedWeight+ VisceraWeight+ ShellWeight,data=abalone)\nstep.model <- step(model2, direction = \"backward\",trace=FALSE)\nsummary(step.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Rings ~ Type.I + Diameter + WholeWeight + ShuckedWeight + \n    VisceraWeight + ShellWeight, data = abalone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.1597 -1.3126 -0.3417  0.8697 13.7453 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     4.27930    0.26379  16.222  < 2e-16 ***\nType.I         -0.90566    0.08966 -10.101  < 2e-16 ***\nDiameter       12.64787    0.94590  13.371  < 2e-16 ***\nWholeWeight     9.06125    0.72926  12.425  < 2e-16 ***\nShuckedWeight -19.93199    0.81772 -24.375  < 2e-16 ***\nVisceraWeight -10.22104    1.29262  -7.907 3.34e-15 ***\nShellWeight     9.57105    1.12421   8.514  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.206 on 4170 degrees of freedom\nMultiple R-squared:  0.5324,\tAdjusted R-squared:  0.5317 \nF-statistic: 791.3 on 6 and 4170 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Backward elimination using `SignifReg` package\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(SignifReg)\nnullmodel = lm(Rings~1, data=abalone)\nfullmodel = lm(Rings~Type.I + Type.M+ LongestShell+ Diameter+ WholeWeight+ ShuckedWeight+ VisceraWeight+ ShellWeight, data=abalone)\nscope = list(lower=formula(nullmodel),upper=formula(fullmodel))\n\nSub.model=drop1SignifReg(fullmodel, scope, alpha = 0.05, criterion = \"p-value\",override=\"TRUE\")\nsummary(Sub.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Rings ~ Type.I + Type.M + LongestShell + Diameter + \n    WholeWeight + VisceraWeight + ShellWeight, data = abalone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7218 -1.4383 -0.4426  0.8413 16.3379 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    5.19685    0.30571  16.999  < 2e-16 ***\nType.I        -1.08469    0.10933  -9.921  < 2e-16 ***\nType.M        -0.07688    0.08937  -0.860   0.3897    \nLongestShell  -3.36753    1.93732  -1.738   0.0822 .  \nDiameter      13.40996    2.37930   5.636 1.85e-08 ***\nWholeWeight   -5.61381    0.43643 -12.863  < 2e-16 ***\nVisceraWeight -1.87624    1.33974  -1.400   0.1615    \nShellWeight   26.79546    0.93284  28.725  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.357 on 4169 degrees of freedom\nMultiple R-squared:  0.4663,\tAdjusted R-squared:  0.4654 \nF-statistic: 520.3 on 7 and 4169 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Forward selection\n\n1.  Choose the significance level (SL = 0.05).\n2.  Fit all possible simple regression models and select the one with the lowest P-value.\n3.  Keep this variable and fit all possible models with one extra predictor added to the one you already have.\n4.  Find the predictor with the lowest P-value. If P \\< Sl, go back to step 3. Otherwise, you're done!\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <-  lm(Rings~Type.I + Type.M+ LongestShell+ Diameter+ WholeWeight+ ShuckedWeight+ VisceraWeight+ ShellWeight,data=abalone)\nstep.model2 <- step(model2, direction = \"forward\",trace=FALSE)\nsummary(step.model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Rings ~ Type.I + Type.M + LongestShell + Diameter + \n    WholeWeight + ShuckedWeight + VisceraWeight + ShellWeight, \n    data = abalone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.1813 -1.3130 -0.3437  0.8690 13.7654 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     4.24786    0.28883  14.707  < 2e-16 ***\nType.I         -0.87919    0.10269  -8.562  < 2e-16 ***\nType.M          0.04437    0.08380   0.529    0.597    \nLongestShell   -0.05352    1.81860  -0.029    0.977    \nDiameter       12.73487    2.22738   5.717 1.16e-08 ***\nWholeWeight     9.06362    0.72947  12.425  < 2e-16 ***\nShuckedWeight -19.95631    0.82169 -24.287  < 2e-16 ***\nVisceraWeight -10.18938    1.29997  -7.838 5.76e-15 ***\nShellWeight     9.57238    1.12490   8.510  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.207 on 4168 degrees of freedom\nMultiple R-squared:  0.5324,\tAdjusted R-squared:  0.5315 \nF-statistic: 593.3 on 8 and 4168 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Forward selection using `SignifReg` package\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(SignifReg)\nnullmodel = lm(Rings~1, data=abalone)\nfullmodel = lm(Rings~Type.I + Type.M+ LongestShell+ Diameter+ WholeWeight+ ShuckedWeight+ VisceraWeight+ ShellWeight, data=abalone)\nscope = list(lower=formula(nullmodel),upper=formula(fullmodel))\n\nsub.model2=add1SignifReg(nullmodel,scope, alpha = 0.05, criterion = \"p-value\",override=\"TRUE\")\nsummary(sub.model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Rings ~ LongestShell, data = abalone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9665 -1.6961 -0.7423  0.8733 16.6776 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    2.1019     0.1855   11.33   <2e-16 ***\nLongestShell  14.9464     0.3452   43.30   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.679 on 4175 degrees of freedom\nMultiple R-squared:  0.3099,\tAdjusted R-squared:  0.3098 \nF-statistic:  1875 on 1 and 4175 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Stepwise regression\n\n1.  Select a significance level to enter and a significance level to stay.\n\n2.  Perform the next step of forward selection where you add the new variable.\n\n3.  Now perform all of the steps of backward elimination.\n\n4.  Now head back to step two, then move forward to step 3, and so on until no new variables can enter and no new variables can exit.\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <-  lm(Rings~Type.I + Type.M+ LongestShell+ Diameter+ WholeWeight+ ShuckedWeight+ VisceraWeight+ ShellWeight,data=abalone)\nstep.model2 <- step(model2, direction = \"forward\", trace=FALSE)\nsummary(step.model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Rings ~ Type.I + Type.M + LongestShell + Diameter + \n    WholeWeight + ShuckedWeight + VisceraWeight + ShellWeight, \n    data = abalone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.1813 -1.3130 -0.3437  0.8690 13.7654 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     4.24786    0.28883  14.707  < 2e-16 ***\nType.I         -0.87919    0.10269  -8.562  < 2e-16 ***\nType.M          0.04437    0.08380   0.529    0.597    \nLongestShell   -0.05352    1.81860  -0.029    0.977    \nDiameter       12.73487    2.22738   5.717 1.16e-08 ***\nWholeWeight     9.06362    0.72947  12.425  < 2e-16 ***\nShuckedWeight -19.95631    0.82169 -24.287  < 2e-16 ***\nVisceraWeight -10.18938    1.29997  -7.838 5.76e-15 ***\nShellWeight     9.57238    1.12490   8.510  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.207 on 4168 degrees of freedom\nMultiple R-squared:  0.5324,\tAdjusted R-squared:  0.5315 \nF-statistic: 593.3 on 8 and 4168 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n# Model Diagnosis\n\n-   The model fitting is just the first part of the regression analysis since this is all based on certain assumptions.\n\n-   Regression diagnostics are used to evaluate the model assumptions and investigate whether or not there are observations with a large, undue influence on the analysis.\n\n## Model Assumptions\n\nBefore using a regression model for interpreting the data, we must check that the model assumptions are met.\n\nBasic assumptions of regression models:\n\n-   Linearity of the data: The relationship between the predictor (x) and the outcome (y) is assumed to be linear.\n\n-   Normality of residuals: The residual errors are assumed to be normally distributed.\n\n-   Homogeneity of residuals variance: The residuals are assumed to have a constant variance (homoscedasticity)\n\n-   Independence of residuals error terms.\n\n## Checking Assumptions\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2)) \nplot(step.model2)\n```\n\n::: {.cell-output-display}\n![](lecture5_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### Using `ggfortify` package\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2)) \nlibrary(ggfortify) \nautoplot(step.model2)\n```\n\n::: {.cell-output-display}\n![](lecture5_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nThe diagnostic plots show residuals in four different ways:\n\n1.  **Residuals vs Fitted**: to check the linear relationship assumptions.\n\n2.  **Normal Q-Q**: to examine whether the residuals are normally distributed.\n\n3.  **Scale-Location** (or Spread-Location): to check the homogeneity of variance of the residuals (homoscedasticity).\n\n4.  **Residuals vs Leverage**: to identify extreme values that might influence the regression analysis.\n\n## Identifying Influential Outliers\n\n-   An outlier is a point which is often distant from the others.\n\n-   It is a point with a large value of its residual.\n\n-   Different residual measures can be used to identify outliers.\n\n    -   Standardised residuals\n\n    -   Studentized residuals\n\n    -   Cook's distance\n\n------------------------------------------------------------------------\n\n### Standardised residuals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstandard_res=rstandard(step.model2)\nplot(fitted(step.model2), standard_res)\n```\n\n::: {.cell-output-display}\n![](lecture5_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n-   Standardized residuals are \"mostly\" between 2 and 2, but they are dependent.\n\n------------------------------------------------------------------------\n\n### Studentized residuals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudent_res=rstudent(step.model2)\nplot(fitted(step.model2), student_res)\n```\n\n::: {.cell-output-display}\n![](lecture5_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n-   For an outlier the absolute values of its Studentized residual is greater than the 0.975th quantile of the Student distribution with $n-p-2$ degrees of freedom\n\n------------------------------------------------------------------------\n\n### Cook's Distance\n\n-   It is calculated by removing the $i$th data point from the model and refitting the regression model.\n\n-   It summarizes how much all the values in the regression model change when the $i$th observation is removed.\n\n-   The formula for Cook's distance is:\n\n    ![](images/paste-0464CED3.png){width=\"336\"}\n\n-   An R function to calculate Cook's distance is `cooks.distance()`.\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndist_cook=cooks.distance(step.model2)\nplot(dist_cook, type=\"h\")\n```\n\n::: {.cell-output-display}\n![](lecture5_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n## Checking for multicollinearity\n\n-   It is a statistical terminology where more than one independent variable is correlated with each other.\n\n-   Results in reducing the reliability of statistical inferences.\n\n-   Hence, these variables must be removed when building a multiple regression model.\n\n-   Variance inflation factor (VIF) is used for detecting the multicollinearity in a model.\n\n------------------------------------------------------------------------\n\n### Variance inflation factor\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\nvif_values <- vif(step.model2)          \nbarplot(vif_values, main = \"VIF Values\", horiz = TRUE, col = \"steelblue\") \nabline(v = 5, lwd = 3, lty = 2)    \n```\n\n::: {.cell-output-display}\n![](lecture5_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n## Testing the multicollinearity among the predictor variables\n\n-   The `mctest` package in R provides the Farrar-Glauber test and other relevant tests for multicollinearity.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mctest)\nomcdiag(mod=step.model2)         \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nomcdiag(mod = step.model2)\n\n\nOverall Multicollinearity Diagnostics\n\n                       MC Results detection\nDeterminant |X'X|:         0.0000         1\nFarrar Chi-Square:     62488.4404         1\nRed Indicator:             0.7378         1\nSum of Lambda Inverse:   262.8746         1\nTheil's Method:            2.8529         1\nCondition Number:        104.1100         1\n\n1 --> COLLINEARITY is detected by the test \n0 --> COLLINEARITY is not detected by the test\n```\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mctest)\nimcdiag(mod=step.model2)         \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nimcdiag(mod = step.model2)\n\n\nAll Individual Multicollinearity Diagnostics Result\n\n                   VIF    TOL         Wi         Fi Leamer     CVIF Klein\nType.I          1.9723 0.5070   579.0942   675.7720 0.7120  -0.9408     0\nType.M          1.3975 0.7155   236.7649   276.2919 0.8459  -0.6666     0\nLongestShell   40.9040 0.0244 23765.6736 27733.2698 0.1564 -19.5107     1\nDiameter       41.9003 0.0239 24359.0324 28425.6879 0.1545 -19.9859     1\nWholeWeight   109.7357 0.0091 64759.8525 75571.2838 0.0955 -52.3425     1\nShuckedWeight  28.5255 0.0351 16393.4264 19130.2517 0.1872 -13.6063     1\nVisceraWeight  17.4123 0.0574  9774.6981 11406.5499 0.2396  -8.3054     1\nShellWeight    21.0270 0.0476 11927.5022 13918.7571 0.2181 -10.0296     1\n                IND1   IND2\nType.I        0.0009 0.5994\nType.M        0.0012 0.3458\nLongestShell  0.0000 1.1861\nDiameter      0.0000 1.1868\nWholeWeight   0.0000 1.2047\nShuckedWeight 0.0001 1.1732\nVisceraWeight 0.0001 1.1460\nShellWeight   0.0001 1.1580\n\n1 --> COLLINEARITY is detected by the test \n0 --> COLLINEARITY is not detected by the test\n\nType.M , LongestShell , coefficient(s) are non-significant may be due to multicollinearity\n\nR-square of y on all x: 0.5324 \n\n* use method argument to check which regressors may be the reason of collinearity\n===================================\n```\n:::\n:::\n\n\n# Model Selection\n\n-   Model selection criteria refer to a set of exploratory tools for improving regression models.\n\n-   Each model selection tool involves selecting a subset of possible predictor variables that still account well for the variation in the regression model's response variable.\n\n-   This suggests that we need a quality criteria that considers the size of the model, since our preference is for small models that still fit well.\n\n-   Several criteria are available to measure the performance of model. (AIC, BIC , $R^2$ (Coefficient of determination), adjacent $R^2$)\n\n------------------------------------------------------------------------\n\n-   The `summary()` function gives both $R^2$ and adjacent $R^2$ measures together.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(step.model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Rings ~ Type.I + Type.M + LongestShell + Diameter + \n    WholeWeight + ShuckedWeight + VisceraWeight + ShellWeight, \n    data = abalone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.1813 -1.3130 -0.3437  0.8690 13.7654 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     4.24786    0.28883  14.707  < 2e-16 ***\nType.I         -0.87919    0.10269  -8.562  < 2e-16 ***\nType.M          0.04437    0.08380   0.529    0.597    \nLongestShell   -0.05352    1.81860  -0.029    0.977    \nDiameter       12.73487    2.22738   5.717 1.16e-08 ***\nWholeWeight     9.06362    0.72947  12.425  < 2e-16 ***\nShuckedWeight -19.95631    0.82169 -24.287  < 2e-16 ***\nVisceraWeight -10.18938    1.29997  -7.838 5.76e-15 ***\nShellWeight     9.57238    1.12490   8.510  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.207 on 4168 degrees of freedom\nMultiple R-squared:  0.5324,\tAdjusted R-squared:  0.5315 \nF-statistic: 593.3 on 8 and 4168 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAIC(step.model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 18477.14\n```\n:::\n\n```{.r .cell-code}\nlibrary(lme4)\nBIC(step.model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 18540.52\n```\n:::\n:::\n\n\n-   Choose the model with lower AIC and BIC values.\n\n------------------------------------------------------------------------\n\n## In-Class Assignment\n\nObtain a multiple linear regression model that best describes the variations in the response variable `Rings` in the `abalone` dataset. Briefly discuss the steps you followed and the suitability of the model you selected.\n\n# Some Useful Links ...\n\n-   <https://www.youtube.com/watch?v=dQNpSa-bq4M>\n\n-   <https://www.youtube.com/watch?v=zyEZop-5K9Q>\n\n-   <https://datascienceplus.com/multicollinearity-in-r/>\n\n-   <https://www.codingprof.com/3-ways-to-test-for-multicollinearity-in-r-examples/>\n",
    "supporting": [
      "lecture5_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}