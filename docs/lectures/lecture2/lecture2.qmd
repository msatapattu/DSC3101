---
title: "DSC 3101 - Advanced Statistical Applications II"
subtitle: "Resampling Methods: Cross-validation"
author: "Dr. Malima Atapattu"
institute: "Department of Statistics & Computer Science"
format: 
  revealjs:
      theme: [serif, custom.scss]
      slide-number: true
      chalkboard: 
        theme: whiteboard
      
editor: visual
---

## Cross-validation

::: incremental
-   Cross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations.

-   It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.

-   Sometimes, this is called *k -- fold cross-validation*.
:::

## Cross-validation

::: incremental
-   The data is broken into *k* (usually five or ten) non-overlapping sections (folds) of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining *k-1* folds.

-   The mean squared error, $MSE$, is then computed on the observations in the held-out fold.
:::

. . .

$$MSE = \dfrac{1}{n} \sum_{i=1}^n (\hat y_i - y_i)^2$$

## Cross-validation



- This procedure is repeated $k$ times; each time, a different group of observations is treated as a validation set. This process results in $k$ estimates of the test error,$MSE_1, MSE_2,\dots , MSE_k$. 

. . .

- The $k$-fold CV estimate is computed by averaging these values, 

. . .

$$CV_{(k)} = \dfrac{1}{k} \sum_{i=1}^k MSE_i$$

## Cross-validation in R - Example 1

::: incremental
-   First, we consider a general framework for running our own cross-validation on models.

-   Consider the *"Auto"* dataset in *ISLR* package $(n = 392)$.
:::

. . .

![](images/Auto.png){fig-align="center"}

## Cross-validation in R - Example 1

- Let’s use $k = 5$ ($5$-fold cross-validation) for the *“Auto”* data set and fit a simple linear regression model for mpg by using horsepower.  

. . .

- Here we break the dataset into 5 groups randomly.

. . .

```{r}
#| echo: true
library(ISLR)
set.seed(2)
data = Auto

attach(data)
k=5 # of folds

#generate folds
folds <- data.frame(Fold=sample(rep(x=1:k, length.out=nrow(data))),Row=1:nrow(data))

MSE<- 0

```


## Cross-validation in R - Example 1

```{r}
#| echo: true


for(i in 1:max(folds$Fold))
{
# rows that are in validation set
valid_set <- folds$Row[folds$Fold == i]

#training set
train_set <- data[-valid_set,]

#model fitting for the training set
lm.fit <- lm(mpg ~ horsepower ,data=train_set )

#predicting the response in validation set by using the fitted model
pred <- predict(lm.fit, data[valid_set, ])

#test errors
MSE[i]= mean((mpg[valid_set]-pred)^2)

}

```

## Cross-validation in R - Example 1

```{r}
#| echo: true

MSE #Test errors 

mean(MSE) # k-fold CV estimate

```

. . . 

Next we fit a polynomial regression model to the data and compare the two models.

## Cross-validation in R – Example 2

- In this example, we use *poly()* and fit a regression model for *mpg* with a quadratic term of horsepower. By using  $5$-fold cross-validation, we measure the accuracy of the predictive model.

. . .

- Follow the same steps we did in example 1 but when fitting the polynomial regression model use,

. . . 

```{r}
#| echo: true
#model fitting for the training set
lm.fit <- lm(mpg ~ poly(horsepower,2) ,data=train_set )

```

## Cross-validation in R – Example 2


```{r}
# polynomial regression model
set.seed(2)
data = Auto

attach(data)
k=5 # of folds

#generate folds
folds <- data.frame(Fold=sample(rep(x=1:k, length.out=nrow(data))),Row=1:nrow(data))

MSE<- 0

for(i in 1:max(folds$Fold))
{
# rows that are in validation set
valid_set <- folds$Row[folds$Fold == i]

#training set
train_set <- data[-valid_set,]

#model fitting for the training set
lm.fit <- lm(mpg ~ poly(horsepower,2) ,data=train_set )

#predicting the response in validation set by using the fitted model
pred <- predict(lm.fit, data[valid_set, ])

#test errors
MSE[i]= mean((mpg[valid_set]-pred)^2)

}

```
- The test errors for 5 folds are: 

. . .

```{r}
MSE #Test errors 
```

. . .

- The 5-fold CV estimate is: 
```{r}
mean(MSE) # k-fold CV estimate

```

## Finding the best model

::: incremental

- According to example 1, the 5-fold CV estimate of the simple linear regression model for *mpg* is $24.22$.

- According to example 2, the 5-fold CV estimate of the regression model with the quadratic term is $19.32$.

- We can conclude that a model which predicts *mpg* using a quadratic function of horsepower performs better than a model that involves only a linear function of horsepower.

::: 

## *"cv.glm()"* for Cross-validation 

- The **boot** package has *cv.glm()* for performing $k$-fold cross-validation. As the name implies, it works only for generalized linear models, which will suffice for a number of situations. 5 polynomial regression models are considered.
```{r}
#| echo: true

library(boot)

#cv.glm function to compute test errors of cross-validation
set.seed(17)
cv.error.5=rep(0 ,5)
for (i in 1:5){
 glm.fit=glm(mpg ~ poly(horsepower ,i),data=Auto)
 #cv error for 10-fold cross-validation using cv.glm()
 cv.error.5[i]=cv.glm(Auto ,glm.fit ,K=10) $delta [1] 
 }
cv.error.5


```
## Exercise
Consider the *"bodyfat"* dataset, available in the R package **mfp**.Find the best regression model by using cross-validation to predict the **density** by using the other variables.(Skip first 3 variables.)
```{r}
#| echo: true


library(mfp)
data(bodyfat)
dim(bodyfat)
head(bodyfat)
```


## Summary

::: incremental

- Cross-validation allows us to get an idea of how well our models will predict in the future.

- Instead of using all of our collected data as our training set, we set aside part of it to serve as simulated ‘new’ data which is called a *validation set* or *test set*.

- Based on the test error estimate we can measure the accuracy of the predictive model.


:::


