---
title: "DSC 3101 - Advanced Statistical Applications II"
subtitle: "Statistical Simulation Methods"
author: "Dr. Malima Atapattu"
institute: "Department of Statistics & Computer Science"
format: 
  revealjs:
      theme: [serif, custom.scss]
      slide-number: true
      chalkboard: 
        theme: whiteboard
      
editor: visual
---

## Simulation

::: incremental
-   An important topic in statistics.
-   A computationally intensive method used to solve difficult problems.
-   Can be used instead of creating physical processes and experimenting with them in order to understand their operational characteristics.
-   Through simulation, a vast number of hypothetical conditions can be quickly and inexpensively examined.
:::

------------------------------------------------------------------------

-   Sometimes, we want to implement a statistical procedure that requires random number generation or sampling from a known distribution. (Uniform, Normal, Poisson, Binomial etc.)

::: incremental
-   Also, can be used to draw from an empirical distribution which is known as *resampling*. Resampling allows us to assess the uncertainty of estimates in complex models. (Bootstrapping, Cross-validation)
:::

## Generating Random Numbers

Some example functions for probability distributions in R:

-   *rnorm*: generate random Normal variates with a given mean and standard deviation

-   *dnorm*: evaluate the Normal probability density (with a given mean/SD) at a point (or vector of points)

-   *pnorm*: evaluate the cumulative distribution function for a Normal distribution

-   *rpois*: generate random Poisson variates with a given rate

## Generating Random Numbers (cont...)

For each probability distribution, there are typically four functions available that start with a

-   *r* - for random number generation,
-   *d* - for density,
-   *p* - for cumulative distribution,
-   *q* - for quantile function (inverse cumulative distribution).

## R Examples:

![](images/norm_d.png)

-   Let's simulate standard Normal random numbers with mean $0$ and standard deviation $1$ (default parameters).

```{r}
#| echo: true


x <- rnorm(10)
x

```

## R Examples:

-   We can modify the default parameters to simulate numbers with mean $20$ and standard deviation $2$.

```{r}
#| echo: true

x <- rnorm(10,20,2)
x

```

-   When $X \sim N(0,1),$ we can find $P(X \le 2)$ as,

```{r}
#| echo: true
#| 
p <- pnorm(2, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
p

```

## R Examples:

-   When $X \sim N(0,1),$ we can find $P(X \ge 2)$ as,

```{r}
#| echo: true
#| 
p <- pnorm(2, mean = 3, sd = 2, lower.tail = FALSE, log.p = FALSE)
p
```

-   $95^{th}$ percentile of standard normal distribution is,

```{r}
#| echo: true
z <- qnorm(0.95, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
z
```

## R Examples:

-   Setting the random number seed with *set.seed()* ensures reproducibility of the sequence of random numbers.

```{r}
#| echo: true
set.seed(1)
rnorm(10)

```

## Exercise:

-   Try to use these *r* commands with other probability distributions too.

```{r}
#| echo: true

rpois(10, 1) ## Counts with a mean of 1 - Poisson distribution


pchisq(2.5, df=2, lower.tail = FALSE) ## cumulative prob. of chi-sq distribution with df 2
```

## Simulating a Linear Model

-   Sometimes we want to simulate values that come from a specific model.

-   For that we need to specify the model first, and then simulate from it using the functions discussed above.

-   Suppose we want to simulate from the following linear model,

$$y = \beta_0 + \beta_1 x + \varepsilon$$ where $\varepsilon \sim N(0, 2^2).$

Assume $x \sim N(0,1^2), \beta_0 = 0.5, \beta_1 = 2.$

## Simulating a Linear MOdel

-   Now, let's simulate values from the model in R.

```{r}
#| echo: true

## Always set your seed!
set.seed(20)

## Simulate predictor variable
x <- rnorm(100)

## Simulate the error term
e <- rnorm(100, 0, 2)

## Compute the outcome via the model
y <- 0.5 + 2 * x + e


```

## Simulating a linear model

![Scatter plot of X,Y](images/plot_x_y.png){fig-align="center"}

## Exersice

-   Assume $x \sim U(0,1)$ and simulate from the above linear model. Then, plot the simulated data. *(Hint: runif)*

## Random Sampling

-   The *sample()* function draws randomly from a specified set of (scalar) objects allowing you to sample from arbitrary distributions of numbers.

```{r}
#| echo: true

set.seed(1)

sample(1:10, 4)

sample(1:10, 4)

```

-   Doesn't have to be numbers

```{r}
#| echo: true
#| 
sample(letters, 5)

```

## Random Sampling

-   Do a random permutation

```{r}
#| echo: true

sample(1:10)

sample(1:10)

```

-   Sample with replacement

```{r}
#| echo: true

sample(1:10, replace = TRUE)


```

## Random Sampling

-   To sample more complicated things, such as rows from a data frame or a list, you can sample the indices into an object rather than the elements of the object itself.

-   For an example, consider the *airquality* dataset.

```{r}
#| echo: true
library(datasets)
data(airquality)

head(airquality)
```

## Random Sampling

-   Now we just need to create the index vector indexing the rows of the data frame and sample directly from that index vector

```{r}
#| echo: true

 set.seed(20)

## Create index vector
idx <- seq_len(nrow(airquality))

## Sample from the index vector
samp <- sample(idx, 6)
samp

airquality[samp, ]
```

## Random Sampling

-   Other more complex objects can be sampled in this way, as long as there's a way to index the sub-elements of the object.

## Resampling Methods

::: incremental
-   Involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model.

-   Computationally expensive, because they involve fitting the same statistical method multiple times using different subsets of the training data.

-   Also, these can be to estimate the precision of a statistic.

-   We discuss two of the most commonly used resampling methods, **bootstrap** and **cross-validation**.
:::

## The Bootstrap

::: incremental
-   A widely applicable and extremely powerful statistical tool that can be used to quantify the uncertainty or variability associated with a given estimator or statistical learning method.

-   This technique resamples the original dataset with replacement to create pseudo-datasets that are similar to, but slightly perturbed from, the original dataset.

-   Particularly useful when the statistic in question does not have a readily accessible formula for its standard error.
:::

## The Bootstrap

::: incremental
-   We start with $n$ rows of data.

-   Some statistic (whether a mean, regression, or some arbitrary function) is applied to the data.

-   Then the data is sampled, creating a new dataset. This new set still has $n$ rows except that there are repeats and other rows are entirely missing.

-   The statistic is applied to this new dataset. The process is repeated $R$ times (typically around $1,000$), which generates an entire distribution for the statistic.

-   This distribution can then be used to find the **mean** and **confidence interval** (typically $95\%$) for the statistic.
:::

## A Graphical Illustration of Bootstrap Approach

![](images/1.JPG){fig-align="center" width="4471"}

## The Bootstrap: Example

Suppose that we wish to invest a fixed sum of money in two financial assets that yield returns of $X$ and $Y$, respectively, where $X$ and $Y$ are random quantities. We will invest a fraction $\alpha$ of our money in $X$, and will invest the remaining $(1-\alpha)$ in $Y.$ Since there is variability associated with the returns on these two assets, we wish to choose $\alpha$ to minimize the total risk, or variance, of our investment.

-   (i.e. we want to minimize $Var(\alpha X+(1-\alpha)Y)$.

## The Bootstrap: Example

-   We can show that the variance will be minimized when $$\alpha = \dfrac{\sigma^2_Y-\sigma_{XY}}{\sigma^2_X+\sigma^2_Y-2\sigma_{XY}}$$ where $\sigma^2_X = Var(X), \sigma^2_Y = Var(Y), \sigma_{XY}=Cov(X,Y)$

## The Bootstrap: Example cont...

::: incremental
-   In reality, the quantities $\sigma^2_X$ ,$\sigma^2_Y$, and $\sigma_{XY}$ are unknown.

-   We can compute estimates for these quantities, $\hat\sigma^2_X$ ,$\hat\sigma^2_Y$, and $\hat\sigma_{XY}$, using a data set that contains past measurements for $X$ and $Y$. We can then estimate the value of $\alpha$ that minimizes the variance of our investment using those estimates.
:::

## The Bootstrap: Example cont...

::: incremental
-   In bootstrap, we obtain $R$ distinct data sets of size $n$ by repeatedly sampling observations from the original data set and estimate $\hat\alpha$ for each dataset. We thereby obtain $R$ estimates for $\alpha$, which we can call $\hat\alpha_1, \hat\alpha_2, \dots , \hat\alpha_R.$

-   To quantify the accuracy of our estimate $\hat\alpha$, we can estimate the standard deviation of $\hat\alpha$ by using $\hat\alpha_1, \hat\alpha_2, \dots , \hat\alpha_R.$
:::

## Bootstrap in R

-   Performing a bootstrap analysis in R involves only two steps.

::: incremental
1.  First, we must create a function that computes the statistic of interest.

2.  Second, we use the *boot()* function, which is part of the *boot* library.
:::

## Bootstrap in R

Consider the "Portfolio" dataset in package "ISLR" containing $100$ returns for each of two assets, $X$ and $Y$. Let's use these data to estimate the optimal fraction $\hat\alpha$ to invest in each asset to minimize investment risk of the combined portfolio.

## Bootstrap in R: Example

```{r}
#| echo: true

library(ISLR)
library(boot)

data(Portfolio)
head(Portfolio)

```

## Bootstrap in R: Example

-   First, we need to write a function to compute the required statistic. For this example, it should compute $\hat\alpha$.

```{r}
#| echo: true

#Function to compute the required statistic
alpha.fn=function (data ,index){
  X=data$X[index]
  Y=data$Y[index]
  return ((var(Y)-cov(X,Y))/(var(X)+var(Y) -2*cov(X,Y)))
}

```

## Bootstrap in R: Example

```{r}
#| echo: true

set.seed(1)
reps <- boot(data = Portfolio , statistic = alpha.fn, R=1000)
reps

```

-   The first argument to *boot()* is the data, second argument is the *function* that is to be computed on the data and third is the *number of bootstrap replicates*.

## Bootstrap in R: Confidence Interval

-   To compute confidence intervals,

*boot.ci(boot_object,conf,type)*

conf -- confidence level (default is 95%)

type -- type of conf. interval ("norm", "basic","stud", "perc", "bca" and "all") -- default is "all"

## Bootstrap in R: Confidence Interval

```{r}
#| echo: True

boot.ci(reps)

```

## Estimating the Accuracy of a Linear Regression Model by Bootstrap

-   Here we use the bootstrap approach in order to assess the variability of the estimates for $\beta_0$ and $\beta_1$, the *intercept* and *slope* terms for a linear regression model.

-   Consider the *"Auto"* dataset in *ISLR* package.

![](images/Auto.png){fig-align="center"}

## Estimating the Accuracy of a Linear Regression Model by Bootstrap

-   Consider the linear regression model that uses **horsepower** to predict **mpg**.

-   We will compute the standard error for each coefficient by using the bootstrap method.

```{r}
#| echo: true

set.seed(1)

#Function to estimate the regression coefficients

coef.fn = function(data,index){
  d <- data[index,]
  fit <- lm(mpg ~ horsepower, data = d)
  return(coef(fit))
}
```

## Estimating the Accuracy of a Linear Regression Model by Bootstrap

```{r}
#| echo: true

coef.fn(Auto,1:392)
```

::: incremental
-   Next, we use the *boot()* function to compute the standard errors of $2000$ bootstrap estimates for the intercept and slope terms.
:::

## Estimating the Accuracy of a Linear Regression Model by Bootstrap

```{r}
#| echo: true

#perform bootstrapping with 2000 replications
reps <- boot(Auto,coef.fn,2000)
reps

```

## Estimating the Accuracy of a Linear Regression Model by Bootstrap

-   The estimated coefficient for the intercept of the model is 39.9359 and the standard error of this estimate is 0.87914.

-   The estimated coefficient for the predictor variable *horsepower* in the model is -0.1578 and the standard error of this estimate is 0.0076.

## Estimating the Accuracy of a Linear Regression Model by Bootstrap

```{r}

plot(reps, index=1) #intercept of the model
```

## Estimating the Accuracy of a Linear Regression Model by Bootstrap

```{r}


plot(reps, index=2) #horsepower predictor variable
```

## Estimating the Accuracy of a Linear Regression Model by Bootstrap

-   We can also calculate 95% confidence intervals for each coefficient:

```{r}
#| echo: true

#Calculate adjusted bootstrap percentile (bca- bias corrected and accelarated) intervals

boot.ci(reps, type='bca', index=1)#intercept of model

```

## Estimating the Accuracy of a Linear Regression Model by Bootstrap

```{r}
#| echo: true

#Calculate adjusted bootstrap percentile (bca) intervals

boot.ci(reps, type='bca', index=2)#horsepower predictor variable

```

## Summary

::: incremental
-   The bootstrap is an incredibly powerful tool that holds a great deal of promise.

-   We can use bootstrap to quantify the uncertainty or variability associated with a given estimator or statistical learning method.

-   The *"boot"* package offers far more than what we have discussed here, including the ability to bootstrap time series and censored data.
:::

## Summary

::: incremental
-   The beautiful thing about bootstrap is its nearly universal applicability. It can be used in just about any situation where an analytical solution is impractical or impossible.

-   There are some instances where the bootstrap is inappropriate, such as for measuring uncertainty of biased estimators like those from the lasso, although such limitations are rare.
:::
